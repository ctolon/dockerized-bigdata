# Spark Cluster
version: '3'
services:

######################################################
# SPARK SERVICES
######################################################

  spark-master:
    image: spark-master
    build: ./spark/spark-master
    container_name: ${SPARK_MASTER_SVC:-spark-master}
    hostname: ${SPARK_MASTER_SVC:-spark-master}
    logging:
      driver: "json-file"
      options:
          max-file: "5"
          max-size: "10m"
    restart: always
    env_file: .env
    #network_mode: host
    networks:
      big-data-network:
        ipv4_address: 172.23.5.2
    ports:
      - "32766:8082"
      - "7077:7077"
      - "32765:7077"
    volumes:
      - ./mnt/spark/apps:/opt/spark-workspace/apps
      - ./mnt/spark/data:/opt/spark-workspace/data

      - ../conf/hive/hive-site.xml:${SPARK_HOME}/conf/hive-site.xml

      - spark-shared-workspace:/opt/spark-workspace
      - spark-logs:${SPARK_HOME}/spark-events

    environment:

      #SPARK_MASTER_HOST: 192.168.3.104
      #SPARK_LOCAL_IP: 192.168.3.104
      #INIT_DAEMON_STEP: setup_spark

      SPARK_MASTER_HOST: 172.23.5.2
      SPARK_LOCAL_IP: 172.23.5.2
      INIT_DAEMON_STEP: setup_spark

    healthcheck:
      test: [ "CMD", "nc", "-z", "spark-master", "8082" ]
      timeout: 45s
      interval: 10s
      retries: 10

  spark-worker:
    image: spark-worker
    build: ./spark/spark-worker
    container_name: ${SPARK_WORKER_SVC:-spark-worker}
    hostname: ${SPARK_WORKER_SVC:-spark-worker}
    logging:
      driver: "json-file"
      options:
          max-file: "5"
          max-size: "10m"
    restart: unless-stopped
    env_file: .env
    depends_on:
      - spark-master
    #network_mode: host
    networks:
      big-data-network:
        ipv4_address: 172.23.5.3
    ports:
      - "32764:8081"
    volumes:
      - ./mnt/spark/apps:/opt/spark-workspace/apps
      - ./mnt/spark/data:/opt/spark-workspace/data

      - ../conf/hive/hive-site.xml:${SPARK_HOME}/conf/hive-site.xml

      - spark-shared-workspace:/opt/spark-workspace
      - spark-logs:${SPARK_HOME}/spark-events
    environment:
      #extra_hosts:
      #  - "spark-master:192.168.3.104"
      #SPARK_MASTER_HOST: 192.168.3.104
      #SPARK_MASTER_PORT: 7077
      #SPARK_MASTER: spark://192.168.3.104:7077
      #SPARK_LOCAL_IP:  192.168.3.104
      #SPARK_LOCAL_HOSTNAME: ${SPARK_WORKER_SVC:-spark-worker}

      SPARK_MASTER_HOST: 172.23.5.2
      SPARK_LOCAL_IP: 172.23.5.3
      SPARK_LOCAL_HOSTNAME: ${SPARK_WORKER_SVC:-spark-worker}

    #tty: true
    #command: /bin/bash


    healthcheck:
      test: [ "CMD", "nc", "-z", "spark-worker", "8081" ]
      timeout: 45s
      interval: 10s
      retries: 10

#  spark-history-server:
#      image: spark-history-server
#      build: ./spark/spark-master
#      container_name: ${SPARK_HS_SVC:-spark-history-server}
#      hostname: ${SPARK_HS_SVC:-spark-history-server}
#      logging:
#        driver: "json-file"
#        options:
#            max-file: "5"
#            max-size: "10m"
#      restart: unless-stopped
#      depends_on:
#        - spark-master
#        - spark-worker
#      # network_mode: host
#      networks:
#        big-data-network:
#          ipv4_address: 172.23.5.4
#      ports:
#        - "18081:18081"
#      volumes:
#      - spark-logs:${SPARK_HOME}/spark-events
#      environment:
#        SPARK_MASTER_HOST: 172.23.5.2
#        SPARK_LOCAL_IP: 172.23.5.4
#        SPARK_LOCAL_HOSTNAME: ${SPARK_HS_SVC:-spark-history-server}
#      healthcheck:
#        test: [ "CMD", "nc", "-z", "spark-history-server", "18081" ]
#        timeout: 45s
#        interval: 10s
#        retries: 10

  jupyter-spark:
    image: jupyter-spark
    build: ./spark/jupyter-spark
    container_name: jupyter-spark
    #ports:
    #  - 8888:8888
    #  - 4040:4040
    #expose:
    #  - "8888"
    depends_on:
      - spark-master
    command: jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token=
    network_mode: host
    #networks:
    #  big-data-network:
    #    ipv4_address: 172.23.5.5
    logging:
      driver: "json-file"
      options:
          max-file: "5"
          max-size: "10m"
    environment:
      SPARK_MASTER_HOST: 172.23.5.2
      SPARK_LOCAL_IP: 127.0.0.1
      SPARK_LOCAL_HOSTNAME: jupyter
    extra_hosts:
      - "spark-master:172.23.5.2"
    volumes:
      - ../conf/hive/hive-site.xml:${SPARK_HOME}/conf/hive-site.xml
      - spark-shared-workspace:/opt/spark-workspace

#  livy:
#    image: livy
#    build: ./livy
#    container_name: livy
#    networks:
#      - big-data-network
#    logging:
#      driver: "json-file"
#      options:
#          max-file: "5"
#          max-size: "10m"
#    restart: always
#    env_file: .env
#    depends_on:
#      - spark-worker
#    ports:
#      - "32758:8998"
#    environment:
#      SPARK_MASTER_ENDPOINT: ${SPARK_MASTER_HOST}
#      SPARK_MASTER_PORT: ${SPARK_MASTER_PORT}
#      DEPLOY_MODE: client
#    healthcheck:
#      test: [ "CMD", "nc", "-z", "livy", "8998" ]
#      timeout: 45s
#      interval: 10s
#      retries: 10


######################################################
# VOLUMES
######################################################

volumes:
  spark-logs:
  spark-shared-workspace:
    name: "hadoop-distributed-file-system"
    driver: local

######################################################
# NETWORK
######################################################

networks:
  big-data-network:
    external: true
    driver: bridge
  